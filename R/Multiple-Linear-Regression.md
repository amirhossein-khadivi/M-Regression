<html>

<body>


<h2 style={font-family: "XB Niloofar">
Multiple Linear Regression
</h2>

<h3 style={font-family: "XB Niloofar">
<br>
Author : Amirhossein Khadivi
<br> <br>
<a href="https://linkedin.com/in/amirhossein-khadivi">Linkedin</a>
<br><br>
<a href="https://github.com/amirhossein-khadivi">Github</a>
<br>
<br>
</h3>

<p>

قبل از شروع پردازش و مدلسازی داده ها تعدادی از کتابخانه های مورد نیاز را
فراخوانی میکنیم.

``` r
library(DAAG)
library(ggplot2)
library(dplyr)
library(car)
library(forecast)
```

داده ها را وارد کرده و در غالب یک دیتافریم نمایش میدهیم.

``` r
c(1.45,1.93,0.81,0.61,1.55,0.95,0.45,1.14,0.74,0.98,1.41,0.81,0.89,0.68,1.39,1.53,0.91,1.49,1.38,1.73,1.11 , 1.68,0.66,0.69,1.98) -> y
c(0.58,0.86,0.29,0.20,0.56,0.28,0.08,0.41,0.22,0.35,0.59,0.22,0.26,0.12,0.65,0.70,0.30,0.70,0.39,0.72,0.45,0.81,0.04,0.20,0.95) -> x1
c(0.71,0.13,0.79,0.20,0.56,0.92,0.01,0.60,0.70,0.73,0.13,0.96,0.27,0.21,0.88,0.30,0.15,0.09,0.17,0.25,0.30,0.32,0.82,0.98,0.00) -> x2

data <- data.frame(y, x1, x2)
data
```

    ##       y   x1   x2
    ## 1  1.45 0.58 0.71
    ## 2  1.93 0.86 0.13
    ## 3  0.81 0.29 0.79
    ## 4  0.61 0.20 0.20
    ## 5  1.55 0.56 0.56
    ## 6  0.95 0.28 0.92
    ## 7  0.45 0.08 0.01
    ## 8  1.14 0.41 0.60
    ## 9  0.74 0.22 0.70
    ## 10 0.98 0.35 0.73
    ## 11 1.41 0.59 0.13
    ## 12 0.81 0.22 0.96
    ## 13 0.89 0.26 0.27
    ## 14 0.68 0.12 0.21
    ## 15 1.39 0.65 0.88
    ## 16 1.53 0.70 0.30
    ## 17 0.91 0.30 0.15
    ## 18 1.49 0.70 0.09
    ## 19 1.38 0.39 0.17
    ## 20 1.73 0.72 0.25
    ## 21 1.11 0.45 0.30
    ## 22 1.68 0.81 0.32
    ## 23 0.66 0.04 0.82
    ## 24 0.69 0.20 0.98
    ## 25 1.98 0.95 0.00

y : متغیر پاسخ <br> x1 , x2 : متغیرهای توضیحی <br> <br> <br>

در آغاز یک مدل خطی میان هر سه متغیر برازش میدهیم.

``` r
fit1 <- lm(data = data , y~.)
fit1
```

    ## 
    ## Call:
    ## lm(formula = y ~ ., data = data)
    ## 
    ## Coefficients:
    ## (Intercept)           x1           x2  
    ##    0.433547     1.652993     0.003945

در خروجی مدل عرض از مبدا(0.433547)،ضریب متغیر توضیحی اول (1.652993) و
ضریب متغیر توضیحی دوم (0.003945) براورد شده است. <br> \*روش براورد
کمترین مربعات خطا میباشد. <br>

``` r
summary(fit1)
```

    ## 
    ## Call:
    ## lm(formula = y ~ ., data = data)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -0.15493 -0.07801 -0.02004  0.04999  0.30112 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) 0.433547   0.065983   6.571 1.31e-06 ***
    ## x1          1.652993   0.095245  17.355 2.53e-14 ***
    ## x2          0.003945   0.074854   0.053    0.958    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.1127 on 22 degrees of freedom
    ## Multiple R-squared:  0.9399, Adjusted R-squared:  0.9344 
    ## F-statistic:   172 on 2 and 22 DF,  p-value: 3.699e-14

در خروجی بالا ابتدا مدل برازش شده و در ادامه چارک های باقیمانده ها
محاسبه شده است. <br> <br> براورد عرض از مبدا(0.433547) ، خطای
استاندارد آن(0.065983)، ضریب متغیر توضیحی اول(1.652993)،خطای
استاندارد آن)0.095245) و ضریب متغیر توضیحی دوم (0.003945) و خطای
استاندارد آن(0.074854) براورد شده است. <br> <br> برای آزمون فرض ضرایب
نیز آماره تی محاسبه شده عرض از مبدا(6.751)و پی-مقدارآن تقریبا صفر
محاسبه شده است که به دلیل کوچکتر بودن پی-مقدارآزمون از سطح معناداری
0.05 فرض جانشین آزمون یعنی مخالف صفر بودن یا لزوم وجود عرض از مبدا در
مدل تایید میشود. <br> <br> آماره تی محاسبه شده برای آزمون فرض متغیر
توضیحی اول(17.355) و پی-مقدار آن تقریبا صفر محاسبه شده است که به دلیل
کوچکتر بودن پی-مقدار آزمون از سطح معناداری 0.05 فرض جانشین آزمون ، یعنی
مخالف صفر بودن یا لزوم وجود متغیر توضیحی اول در مدل تایید مبشود. <br>
<br> همچنین آماره تی محاسبه شده برای آزمون فرض متغیر توضیحی دوم(0.053) و
پی-مقدار آن (0.958) محاسبه شده است که به دلیل بزرگتر بودن پی-مقدار آزمون
از سطح معنارداری 0.05 و بسیار نزدیک بودن آن به یک فرض صفر آزمون یعنی صفر
بودن ضریب یا لزوم نبودن متغیر توضیحی دوم در مدل تایید میشود.(با توجه به
نزدیک صفر بودن براورد ضریب این متغیر و مقدار خطای استاندارد آن که باعث
پوشش دادن صفر توسط ضریب میشد میتوان حدس زد که نیازی به وجود این متغیر
در مدل نیست.) <br> <br> در ادامه تفسیر خروجی ، براورد انحراف معیار
حمله خطا (0.1127) محاسبه شده است. <br> <br> مقدار ضریب تعیین چندگانه
مدل (0.9399) و ضریب تعیین چندگانه تعدیل شده(0.9344) محاسبه شده که تفسیر
ضریب تعیین یعنی اینکه 93.99 درصد از تغییرات متغیر پاسخ توسط مدل بیان
میشود. <br> <br> همچنین برای آزمون فرض مناسبت مدل ، آماره فیشر(172) و
پی-مقدار آن تقریبا صفر محاسبه شده است که به دلیل کوچکتر بودن آن از سطح
معناداری 0.05 فرض جانشین آزمون ، یعنی مخالف صفر بودن ضرایب تایید
میشود. <br> <br> با توجه به عدم لزوم وجود متغیرتوضیحی دوم در مدل با
یه جواب آزمون فرض معناداری آن و تاییدیه های محکم مناسبت مدل بنا بر آزمون
مناسبت مدل و ضریب تعیین چندگانه ، امکان وجود همخطی میان متغیرهای توضیحی
وجود دارد که با محاسبه ماتریس همبستگی داده ها آن را مورد بررسی قرار
میدهیم.

``` r
correlation_matrix <- cor(data)
correlation_matrix
```

    ##             y         x1         x2
    ## y   1.0000000  0.9694776 -0.3420462
    ## x1  0.9694776  1.0000000 -0.3554707
    ## x2 -0.3420462 -0.3554707  1.0000000

``` r
cat('correlation x1 & x2 : ',correlation_matrix[2,3])
```

    ## correlation x1 & x2 :  -0.3554707

با توجه به خروجی ماتریس همبستگی داده ها و همبستگی میان دو متغیر
توضیحی(0.3554707-) وجود همخطی میان متغیرهای توضیحی را رد میکنیم.
<br> البته راه های بصری و استنباطی بیشتری برای بررسی همخطی وجود دارد که
ما در اینجا به بررسی ماتریس همبستگی بسنده میکنیم. <br> <br>

``` r
confint(fit1)
```

    ##                  2.5 %    97.5 %
    ## (Intercept)  0.2967067 0.5703875
    ## x1           1.4554666 1.8505203
    ## x2          -0.1512924 0.1591822

در خروجی بالا یک فاصله اطمینان 95 درصدی برای متغیرها براود شده است که در
اینجا هم با توجه به طویل بودن بازه اطمینان متغیر توضیحی دوم و همچنین
پوشش دادن نقطه صفر توسط بازه اطمینان به عدم لزوم وجود این متغیر در
مدل پی میبریم. <br> <br>

``` r
fit_value10 <- 0.433547 + (1.652993*data$x1) + (0.003945*data$x2)
fit_value11 <- fit1$fitted.values
fit1_value <- data.frame(fit_value10 , fit_value11)
fit1_value
```

    ##    fit_value10 fit_value11
    ## 1    1.3950839   1.3950842
    ## 2    1.8556338   1.8556343
    ## 3    0.9160315   0.9160317
    ## 4    0.7649346   0.7649348
    ## 5    1.3614323   1.3614326
    ## 6    0.9000144   0.9000146
    ## 7    0.5658259   0.5658260
    ## 8    1.1136411   1.1136414
    ## 9    0.7999670   0.7999671
    ## 10   1.0149744   1.0149746
    ## 11   1.4093257   1.4093261
    ## 12   0.8009927   0.8009928
    ## 13   0.8643903   0.8643905
    ## 14   0.6327346   0.6327348
    ## 15   1.5114641   1.5114643
    ## 16   1.5918256   1.5918260
    ## 17   0.9300367   0.9300369
    ## 18   1.5909971   1.5909976
    ## 19   1.0788849   1.0788852
    ## 20   1.6246882   1.6246886
    ## 21   1.1785774   1.1785776
    ## 22   1.7737337   1.7737342
    ## 23   0.5029016   0.5029017
    ## 24   0.7680117   0.7680118
    ## 25   2.0038903   2.0038909

در دیتافریم فوق ، در ستون اول مقادیر برازش شده مدل را با استفاده از
فرمول آن و در ستون دوم با استفاده از تابع آماده نرم افزار محاسبه
کرده ایم که تفاوت ناچیزی میانشان مشاهده میشود که حاصل خطای محاسبات
است. <br> <br> <br>

حال برای بررسی فرض خطی بودن مدل و مشاهده نوع رابطه متغیرهای توضیحی با
متغیر پاسخ ، نمودار متغیرهای اضافه شده را رسم میکنیم.

``` r
avPlots(fit1)
```

![](Multiple-Linear-Regression_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->
<br> با توجه به نمودار سمت چپ یک رابطه خطی تقریبا قوی میان متغیر پاسخ و
متغیر توضیحی اول مشاهده میشود. <br> با توجه به نمودار سمت راست هیچ
رابطه ای اعم از خطی یا غیرخطی میان متغیر پاسخ و متغیر توضیحی دوم
مشاهده نمیشود که دلیل بر عدم لزوم وجود این متغیر در مدل است.

<br> <br> <br> \*\*\*با توجه به استدلال های بصری و استنباطی ای که داشتیم
متغیر توضیحی دوم را از مدل حذف کرده و یک مدل خطی ساده میان متغیرپاسخ و
متغیر توضیحی اول برازش میدهیم.

``` r
fit2 <- lm(data = data , y~x1)
fit2
```

    ## 
    ## Call:
    ## lm(formula = y ~ x1, data = data)
    ## 
    ## Coefficients:
    ## (Intercept)           x1  
    ##      0.4361       1.6512

در خروجی مدل عرض از مبدا(0.4361) و ضریب متغیر توضیحی (1.6512) براورد شده
است. <br> \*روش براورد ، کمترین مربعات خطا است. <br>

``` r
summary(fit2)
```

    ## 
    ## Call:
    ## lm(formula = y ~ x1, data = data)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -0.15633 -0.07633 -0.02145  0.05157  0.29994 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.43609    0.04399   9.913 9.02e-10 ***
    ## x1           1.65121    0.08707  18.963 1.54e-15 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.1102 on 23 degrees of freedom
    ## Multiple R-squared:  0.9399, Adjusted R-squared:  0.9373 
    ## F-statistic: 359.6 on 1 and 23 DF,  p-value: 1.538e-15

در خروجی فوق ، ابتدا مدل برازش شده و سپس چارک های باقیمانده ها را مشاهده
میکنیم. <br> <br> براورد عرض از مبدا(0.43609) ، خطای استاندارد
آن(0.04399) و ضریب متغیر توضیحی (1.65121) ، خطای استاندارد آن(0.08707)
براورد شده است. <br> <br> برای آزمون فرض ضرایب نیز آماره تی محاسبه شده
عرض از مبدا(9.913)و پی-مقدار آن تقریبا صفربراورد شده است که به دلیل
کوچکتر بودن از سطح معناداری 0.05 فرض جانشین آزمون یعنی مخالف صفر بودن
یا لزوم وجود عرض از مبدا در مدل تایید میشود. <br> <br> همچنین آماره تی
محاسبه شده برای آزمون فرض ضریب متغیر توضیحی(18.963) و پی-مقدار آن
تقریبا صفر محاسبه شده است که به دلیل کوچکتر بودن پی-مقدار آزمون از
سطح معناداری 0.05 فرض جانشین آزمون ، یعنی مخالف صفر بودن ضریب یا لزوم
وجود متغیر در مدل تایید میشود. <br> <br> در ادامه تفسیر خروجی فوق ،
براورد انحراف معیار جمله خطا (0.1102) محاسبه شده است. <br> <br>
همچنین ضریب تعیین مدل (0.9399) و ضریب تعیین تعدیل شده آن (0.9373)
براورد شده است که به معنای این است که 93.99 درصد از تغییرات متغیر پاسخ
توسط مدل بیان میشود. <br> <br> در نهایت برای آزمون مناسبت مدل ، آماره
فیشر(359.6) و پی-مقدار آن تقریبا صفر محاسبه شده است که به دلیل کوچکتر
بودن پی-مقدار آزمون از سطح معناداری 0.05 ، فرض جانشین آزمون ، یعنی
مخالف صفر بودن ضریب یا مناسب بودن مدل تایید میشود. <br> <br>

``` r
confint(fit2)
```

    ##                 2.5 %    97.5 %
    ## (Intercept) 0.3450849 0.5270978
    ## x1          1.4710842 1.8313341

در خروجی فوق ، یک فاصله اطمینان 95 درصدی برای عرض از مبدا و متغیر توضیحی
محاسبه شده است. <br> <br>

``` r
fit_value20 <-  0.43609 + (1.65121*data$x1)
fit_value21 <- fit2$fitted.values
fit2_vlue <- data.frame(fit_value20 , fit_value21)
fit2_vlue
```

    ##    fit_value20 fit_value21
    ## 1    1.3937918   1.3937927
    ## 2    1.8561306   1.8561312
    ## 3    0.9149409   0.9149420
    ## 4    0.7663320   0.7663332
    ## 5    1.3607676   1.3607685
    ## 6    0.8984288   0.8984299
    ## 7    0.5681868   0.5681881
    ## 8    1.1130861   1.1130871
    ## 9    0.7993562   0.7993574
    ## 10   1.0140135   1.0140146
    ## 11   1.4103039   1.4103048
    ## 12   0.7993562   0.7993574
    ## 13   0.8654046   0.8654057
    ## 14   0.6342352   0.6342365
    ## 15   1.5093765   1.5093773
    ## 16   1.5919370   1.5919378
    ## 17   0.9314530   0.9314541
    ## 18   1.5919370   1.5919378
    ## 19   1.0800619   1.0800629
    ## 20   1.6249612   1.6249619
    ## 21   1.1791345   1.1791355
    ## 22   1.7735701   1.7735708
    ## 23   0.5021384   0.5021397
    ## 24   0.7663320   0.7663332
    ## 25   2.0047395   2.0047401

در دیتافریم فوق ، در ستون اول مقادیر برازش شده مدل را با استفاده از
فرمول آن و در ستون دوم با استفاده از تابع آماده نرم افزار محاسبه
کرده ایم که تفاوت ناچیزی میانشان مشاهده میشود که حاصل خطای محاسبات
است. <br> <br> <br>

حال که تمام فاکتورهای مناسبت مدل نشان تایید دریافت کردند ، در مرحله آخر
به بررسی و تحلیل مانده ها میپردازیم. <br> به این منظور ابتدا باقیمانده
ها و مقادیر برازش شده مدل را جدا کرده و در غالب یک دیتافریم نمایش
میدهیم.

``` r
resi <- fit2$residuals
fit_value <- fit2$fitted.values

data2 <- data.frame(resi , fit_value)
data2
```

    ##             resi fit_value
    ## 1   0.0562073328 1.3937927
    ## 2   0.0738687696 1.8561312
    ## 3  -0.1049420125 0.9149420
    ## 4  -0.1563331886 0.7663332
    ## 5   0.1892315159 1.3607685
    ## 6   0.0515700790 0.8984299
    ## 7  -0.1181880901 0.5681881
    ## 8   0.0269128890 1.1130871
    ## 9  -0.0593573717 0.7993574
    ## 10 -0.0340145618 1.0140146
    ## 11 -0.0003047588 1.4103048
    ## 12  0.0106426283 0.7993574
    ## 13  0.0245942621 0.8654057
    ## 14  0.0457635437 0.6342365
    ## 15 -0.1193773080 1.5093773
    ## 16 -0.0619377657 1.5919378
    ## 17 -0.0214541040 0.9314541
    ## 18 -0.1019377657 1.5919378
    ## 19  0.2999370721 1.0800629
    ## 20  0.1050380512 1.6249619
    ## 21 -0.0691354772 1.1791355
    ## 22 -0.0935707727 1.7735708
    ## 23  0.1578602761 0.5021397
    ## 24 -0.0763331886 0.7663332
    ## 25 -0.0247400543 2.0047401

<br>

``` r
data2 %>% ggplot(aes(sample = resi)) + stat_qq(color = 'black') + stat_qq_line(color = 'blue') + theme_test()
```

![](Multiple-Linear-Regression_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->
<br> نمودار فوق یک نمودار چندک چندک برای مطابقت داده ها (مانده ها) با
توزیع نرمال است که بر اساس توزیع تجربی داده ها رسم شده است و خط
نمودار نیز بر اساس توزیع نرمال با میانگین صفر و واریانس یک رسم شده
است و هرچه که نقاط رسم شده به خط ترسیم شده نزدیک باشد و منطبق بر آن باشد
فرض نرمال بودن داده ها با قوت بیشتری تایید میشود. <br> در نمودار فوق فرض
نرمال بودن مانده های مدل تایید میشود گرچه داده ی پرت در نمودار مشاهده
میشود. <br> <br> برای اطمینان از این ادعا به استنباط آماری درباره این
موضوع میپردازیم.

``` r
ks.test(data2$resi , 'pnorm' , mean(data2$resi) , sd(data2$resi))
```

    ## 
    ##  One-sample Kolmogorov-Smirnov test
    ## 
    ## data:  data2$resi
    ## D = 0.10883, p-value = 0.8976
    ## alternative hypothesis: two-sided

``` r
shapiro.test(data2$resi)
```

    ## 
    ##  Shapiro-Wilk normality test
    ## 
    ## data:  data2$resi
    ## W = 0.93532, p-value = 0.1154

با توجه به خروجی آزمون های فرض کولموگروف اسمیرنف و شاپیرو ویلک ،به دلیل
بزرگتر بودن پی-مقدار دو آزمون از سطح معناداری 0.05، فرض صفر هر دو آزمون
یعنی نرمال بودن توزیع داده ها(مانده ها) تایید میشود. <br> <br>

``` r
data2 %>% ggplot(aes(x = fit_value , y = resi)) + geom_point(color = 'black') +
        theme_test() + labs(x = 'y^' , y = 'residual') + geom_hline(yintercept = c(0) , color = 'green') + 
  geom_hline(yintercept = c(-0.3,0.3) , color = 'blue')
```

![](Multiple-Linear-Regression_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->
<br> نمودار فوق یک نمودار نقطه ای است. که محور عمودی آن مقادیر استاندارد
شده مانده ها و محور افقی آن مقادیر استاندارد شده پیش بینی مدل هستند. از
این نمودار برای بررسی فرض خطی بودن مدل و همگنی واریانس خطا استفاده
میکنیم. <br> با توجه به اینکه نقاط نمودار در یک نوار افقی و حول
محور صفر به صورت تصادفی و بدون داشتن الگویی خاص توزیع شده اند ،
بنابراین فرض خطی بودن مدل تایید میشود. <br> با توجه به اینکه
پراکندگی نقاط نمودار در سرتاسر محور افقی یکسان است و الگوی خاصی
ندارد ، بنابراین فرض همگنی واریانس خطاها نیز تایید میشود. <br> <br>

در آخر هم فرض ناهمبسته بودن خطاها را بررسی میکنیم.

``` r
resid <- ts(data2$resi)
ggAcf(resid , color = 'black') + theme_test()
```

![](Multiple-Linear-Regression_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->
<br> در سلول کد بالا باقیمانده ها را به سری زمانی تبدیل کرده و سپس
نمودار خودهمبسگی آن را رسم میکنیم. <br> تمامی لگ های نمودار داخل
بازه اطمینانمربوطه هستند به جز لگ شماره 2 که قابل چشم پوشی است ،
بنابراین نتیجه میگیریم که مانده ها ناهمبسته هستند.

<br> <br> برای اطمینان از ناهمبسته بودن خطاها از آماره دوربین واتسون نیز
کمک میگیریم.

``` r
durbinWatsonTest(data2$resi)
```

    ## [1] 2.228521

مقدار این آماره همواره بین 0 تا 4 قرا میگیرد. اگر همبستگی بین مانده ها
وجود نداشته باشد باید آماره نزدیک به 2 باشد ، اگر نزدیک به 0 باشد نشان
دهنده همبستگی مثبت و اگر نزدیک به 4 باشد نشان دهنده هبستگی منفی است. در
مجموع اگر این آماره بین 1.5 تا 2.5 باشد قابل قبول است. <br> حال با توجه
به اینکه مقدار آماره دوربین واتسون مدل(2.228521) کوچکتر از2.5 و نزدیک به
2 است ، بنابراین فرض ناهمبسته بودن مانده های مدل تایید میشود.

<br> <br> <br> <br> \*نتیجه گیری تحلیل مانده ها: <br> هر چهار فرض بنیادی
در تحلیل مانده ها تایید میشوند. <br> 1-فرض خطی بودن تابع رگرسیونی <br>
2-فرض همگن بودن واریانس خطاها <br> 3-فرض ناهمبسته بودن خطاها <br> 4-فرض
نرمال بودن خطاها <br>

<br> <br> \* نتیجه گیری نهایی <br> مدل زیر مورد تایید و معتبر است: <br>
y = 0.4361 + 1.6512\*x1 <br> <br>

</p>

</html>
